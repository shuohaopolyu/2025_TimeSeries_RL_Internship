{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shuo\\miniconda3\\envs\\dcbo\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from hamilton_neural_network import (\n",
    "    TrainTestData,\n",
    "    HamiltonianNeuralNetwork,\n",
    "    LatentHamiltonianNeuralNetwork,\n",
    ")\n",
    "from hamilton_system import HamiltonianSystem\n",
    "from pdf_models import NegLogIndepedentGaussians,NegLogTenDimRosenbrock\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from no_u_turn.nuts import NoUTurnSampling\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = NegLogTenDimRosenbrock()\n",
    "K = NegLogIndepedentGaussians(\n",
    "    tf.constant([0.0] * 10), tf.constant([1.0] * 10)\n",
    ")\n",
    "q0 = tf.constant([[0.0] * 10])\n",
    "p0 = tf.random.normal(q0.shape)\n",
    "T = 120.0\n",
    "leap_frog_per_unit = 40\n",
    "num_samples = 40\n",
    "num_train = int(0.9 * num_samples * leap_frog_per_unit * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_data = TrainTestData(\n",
    "#     num_samples, T, leap_frog_per_unit, q0, p0, U=U, K=K\n",
    "# )\n",
    "# samples = train_test_data()\n",
    "# tf.io.write_file(\"../exps/demo5_train_test_data.txt\", tf.io.serialize_tensor(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172800, 40) (19240, 40)\n",
      "WARNING:tensorflow:From c:\\Users\\shuo\\miniconda3\\envs\\dcbo\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\shuo\\miniconda3\\envs\\dcbo\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:189: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Training started...\n",
      "Epoch 0: Train loss 14.702108383178711, Test loss 15.484529495239258.\n",
      "Epoch 5: Train loss 14.597930908203125, Test loss 15.388049125671387.\n",
      "Epoch 10: Train loss 13.99927043914795, Test loss 14.772724151611328.\n",
      "Epoch 15: Train loss 11.597268104553223, Test loss 12.178045272827148.\n",
      "Epoch 20: Train loss 9.91280746459961, Test loss 10.480630874633789.\n",
      "Epoch 25: Train loss 8.951151847839355, Test loss 9.49661636352539.\n",
      "Epoch 30: Train loss 7.815230846405029, Test loss 8.351136207580566.\n",
      "Epoch 35: Train loss 7.013493061065674, Test loss 7.535499095916748.\n",
      "Epoch 40: Train loss 6.464432239532471, Test loss 6.974184513092041.\n",
      "Epoch 45: Train loss 6.042809963226318, Test loss 6.5227952003479.\n",
      "Epoch 50: Train loss 5.689512729644775, Test loss 6.141939163208008.\n",
      "Epoch 55: Train loss 5.389033317565918, Test loss 5.8275837898254395.\n",
      "Epoch 60: Train loss 5.096538543701172, Test loss 5.52370023727417.\n",
      "Epoch 65: Train loss 4.788365364074707, Test loss 5.204496383666992.\n",
      "Epoch 70: Train loss 4.437258720397949, Test loss 4.852961540222168.\n",
      "Epoch 75: Train loss 4.040559768676758, Test loss 4.471314430236816.\n",
      "Epoch 80: Train loss 3.6424379348754883, Test loss 4.063375473022461.\n",
      "Epoch 85: Train loss 3.2144196033477783, Test loss 3.618093729019165.\n",
      "Epoch 90: Train loss 2.838249444961548, Test loss 3.234227180480957.\n",
      "Epoch 95: Train loss 2.5742952823638916, Test loss 2.961792469024658.\n",
      "Epoch 100: Train loss 2.3611016273498535, Test loss 2.740051746368408.\n",
      "Epoch 105: Train loss 2.191197633743286, Test loss 2.5463860034942627.\n",
      "Epoch 110: Train loss 2.0628104209899902, Test loss 2.377129316329956.\n",
      "Epoch 115: Train loss 1.9592498540878296, Test loss 2.231370687484741.\n",
      "Epoch 120: Train loss 1.8659216165542603, Test loss 2.105849027633667.\n",
      "Epoch 125: Train loss 1.7748407125473022, Test loss 1.9972976446151733.\n",
      "Epoch 130: Train loss 1.6859655380249023, Test loss 1.9008973836898804.\n",
      "Epoch 135: Train loss 1.601863145828247, Test loss 1.8125373125076294.\n",
      "Epoch 140: Train loss 1.5235562324523926, Test loss 1.7303274869918823.\n",
      "Epoch 145: Train loss 1.4512385129928589, Test loss 1.653365135192871.\n",
      "Epoch 150: Train loss 1.3851287364959717, Test loss 1.5812876224517822.\n",
      "Epoch 155: Train loss 1.3256933689117432, Test loss 1.5141613483428955.\n",
      "Epoch 160: Train loss 1.2730365991592407, Test loss 1.452282190322876.\n",
      "Epoch 165: Train loss 1.2265113592147827, Test loss 1.3958011865615845.\n",
      "Epoch 170: Train loss 1.184760570526123, Test loss 1.3444087505340576.\n",
      "Epoch 175: Train loss 1.1460455656051636, Test loss 1.2971224784851074.\n",
      "Epoch 180: Train loss 1.1092230081558228, Test loss 1.2526954412460327.\n",
      "Epoch 185: Train loss 1.0741750001907349, Test loss 1.2104742527008057.\n",
      "Epoch 190: Train loss 1.0411070585250854, Test loss 1.1703425645828247.\n",
      "Epoch 195: Train loss 1.0100579261779785, Test loss 1.132236361503601.\n",
      "Epoch 200: Train loss 0.9808846116065979, Test loss 1.0959804058074951.\n",
      "Epoch 205: Train loss 0.9534522891044617, Test loss 1.061517357826233.\n",
      "Epoch 210: Train loss 0.9276779890060425, Test loss 1.0289306640625.\n",
      "Epoch 215: Train loss 0.9033279418945312, Test loss 0.9980946779251099.\n",
      "Epoch 220: Train loss 0.8801490664482117, Test loss 0.9688645005226135.\n",
      "Epoch 225: Train loss 0.8579543232917786, Test loss 0.941149115562439.\n",
      "Epoch 230: Train loss 0.8366310596466064, Test loss 0.9148795008659363.\n",
      "Epoch 235: Train loss 0.8161793351173401, Test loss 0.8899930715560913.\n",
      "Epoch 240: Train loss 0.7967714071273804, Test loss 0.8663802742958069.\n",
      "Epoch 245: Train loss 0.7787143588066101, Test loss 0.8437156677246094.\n",
      "Epoch 250: Train loss 0.7620608806610107, Test loss 0.8213176131248474.\n",
      "Epoch 255: Train loss 0.7460525035858154, Test loss 0.7988353371620178.\n",
      "Epoch 260: Train loss 0.730002224445343, Test loss 0.7769761681556702.\n",
      "Epoch 265: Train loss 0.714084267616272, Test loss 0.7564469575881958.\n",
      "Epoch 270: Train loss 0.6988694667816162, Test loss 0.737463653087616.\n",
      "Epoch 275: Train loss 0.6846330761909485, Test loss 0.7198581695556641.\n",
      "Epoch 280: Train loss 0.6716848611831665, Test loss 0.7034260630607605.\n",
      "Epoch 285: Train loss 0.6604015231132507, Test loss 0.6880947947502136.\n",
      "Epoch 290: Train loss 0.65073561668396, Test loss 0.6737885475158691.\n",
      "Epoch 295: Train loss 0.6423159837722778, Test loss 0.6604138612747192.\n",
      "Epoch 300: Train loss 0.63485187292099, Test loss 0.647895097732544.\n",
      "Epoch 305: Train loss 0.6281488537788391, Test loss 0.6361425518989563.\n",
      "Epoch 310: Train loss 0.6220406889915466, Test loss 0.6250504851341248.\n",
      "Epoch 315: Train loss 0.6164456605911255, Test loss 0.6145061254501343.\n",
      "Epoch 320: Train loss 0.6113483905792236, Test loss 0.6043856739997864.\n",
      "Epoch 325: Train loss 0.6067338585853577, Test loss 0.5945689082145691.\n",
      "Epoch 330: Train loss 0.6025528311729431, Test loss 0.5849499106407166.\n",
      "Epoch 335: Train loss 0.5987503528594971, Test loss 0.5754532814025879.\n",
      "Epoch 340: Train loss 0.5952070355415344, Test loss 0.5660282373428345.\n",
      "Epoch 345: Train loss 0.5917813777923584, Test loss 0.5566663146018982.\n",
      "Epoch 350: Train loss 0.5883010625839233, Test loss 0.5473878383636475.\n",
      "Epoch 355: Train loss 0.5845814347267151, Test loss 0.5382370352745056.\n",
      "Epoch 360: Train loss 0.5804358124732971, Test loss 0.5292673707008362.\n",
      "Epoch 365: Train loss 0.5756189823150635, Test loss 0.5205183625221252.\n",
      "Epoch 370: Train loss 0.5698221325874329, Test loss 0.5120115876197815.\n",
      "Epoch 375: Train loss 0.5625666975975037, Test loss 0.5037374496459961.\n",
      "Epoch 380: Train loss 0.5529907941818237, Test loss 0.4956510365009308.\n",
      "Epoch 385: Train loss 0.5391122698783875, Test loss 0.487620085477829.\n",
      "Epoch 390: Train loss 0.5141138434410095, Test loss 0.479194700717926.\n",
      "Epoch 395: Train loss 0.44585976004600525, Test loss 0.4668230712413788.\n",
      "Epoch 400: Train loss 0.4253672659397125, Test loss 0.44331005215644836.\n",
      "Epoch 405: Train loss 0.4200194478034973, Test loss 0.43631574511528015.\n",
      "Epoch 410: Train loss 0.4140668511390686, Test loss 0.42938002943992615.\n",
      "Epoch 415: Train loss 0.40880218148231506, Test loss 0.4228556752204895.\n",
      "Epoch 420: Train loss 0.4041031002998352, Test loss 0.416749507188797.\n",
      "Epoch 425: Train loss 0.39965561032295227, Test loss 0.41091781854629517.\n",
      "Epoch 430: Train loss 0.39545464515686035, Test loss 0.4053041934967041.\n",
      "Epoch 435: Train loss 0.3915523588657379, Test loss 0.39985352754592896.\n",
      "Epoch 440: Train loss 0.3879639506340027, Test loss 0.3944850265979767.\n",
      "Epoch 445: Train loss 0.38465285301208496, Test loss 0.3890981674194336.\n",
      "Epoch 450: Train loss 0.38156184554100037, Test loss 0.38360795378685.\n",
      "Epoch 455: Train loss 0.3786506652832031, Test loss 0.37799984216690063.\n",
      "Epoch 460: Train loss 0.3758884370326996, Test loss 0.3723576068878174.\n",
      "Epoch 465: Train loss 0.37328681349754333, Test loss 0.36684560775756836.\n",
      "Epoch 470: Train loss 0.37094250321388245, Test loss 0.36165982484817505.\n",
      "Epoch 475: Train loss 0.36893168091773987, Test loss 0.35708048939704895.\n",
      "Epoch 480: Train loss 0.36611679196357727, Test loss 0.35401278734207153.\n",
      "Epoch 485: Train loss 0.3487532138824463, Test loss 0.35564330220222473.\n",
      "Epoch 490: Train loss 0.38144055008888245, Test loss 0.35978496074676514.\n",
      "Epoch 495: Train loss 0.3746899366378784, Test loss 0.3586665093898773.\n",
      "Epoch 500: Train loss 0.3689012825489044, Test loss 0.35605961084365845.\n",
      "Epoch 505: Train loss 0.3625827133655548, Test loss 0.35306450724601746.\n",
      "Epoch 510: Train loss 0.3560452163219452, Test loss 0.34981414675712585.\n",
      "Epoch 515: Train loss 0.34957125782966614, Test loss 0.3463329076766968.\n",
      "Epoch 520: Train loss 0.34323379397392273, Test loss 0.3426394760608673.\n",
      "Epoch 525: Train loss 0.33707526326179504, Test loss 0.3387466073036194.\n",
      "Epoch 530: Train loss 0.3311070203781128, Test loss 0.33465978503227234.\n",
      "Epoch 535: Train loss 0.3252952992916107, Test loss 0.33037069439888.\n",
      "Epoch 540: Train loss 0.31957435607910156, Test loss 0.32588455080986023.\n",
      "Epoch 545: Train loss 0.31382787227630615, Test loss 0.32126083970069885.\n",
      "Epoch 550: Train loss 0.30799633264541626, Test loss 0.31671011447906494.\n",
      "Epoch 555: Train loss 0.302198588848114, Test loss 0.3125295341014862.\n",
      "Epoch 560: Train loss 0.29667961597442627, Test loss 0.3087870180606842.\n",
      "Epoch 565: Train loss 0.2915525436401367, Test loss 0.3052298426628113.\n",
      "Epoch 570: Train loss 0.28680360317230225, Test loss 0.3016279935836792.\n",
      "Epoch 575: Train loss 0.2823410928249359, Test loss 0.29794543981552124.\n",
      "Epoch 580: Train loss 0.2781039774417877, Test loss 0.29428067803382874.\n",
      "Epoch 585: Train loss 0.2740657329559326, Test loss 0.29069581627845764.\n",
      "Epoch 590: Train loss 0.27020782232284546, Test loss 0.287188857793808.\n",
      "Epoch 595: Train loss 0.26652708649635315, Test loss 0.28374895453453064.\n",
      "Epoch 600: Train loss 0.2630254626274109, Test loss 0.2803795635700226.\n",
      "Epoch 605: Train loss 0.25968846678733826, Test loss 0.27707967162132263.\n",
      "Epoch 610: Train loss 0.25651150941848755, Test loss 0.2738511264324188.\n",
      "Epoch 615: Train loss 0.2534826397895813, Test loss 0.27069544792175293.\n",
      "Epoch 620: Train loss 0.2505965828895569, Test loss 0.2676107585430145.\n",
      "Epoch 625: Train loss 0.24784503877162933, Test loss 0.26459819078445435.\n",
      "Epoch 630: Train loss 0.24521750211715698, Test loss 0.26165464520454407.\n",
      "Epoch 635: Train loss 0.24271059036254883, Test loss 0.2587830722332001.\n",
      "Epoch 640: Train loss 0.2403183877468109, Test loss 0.25598007440567017.\n",
      "Epoch 645: Train loss 0.23803386092185974, Test loss 0.2532447278499603.\n",
      "Epoch 650: Train loss 0.23585358262062073, Test loss 0.25057756900787354.\n",
      "Epoch 655: Train loss 0.23377171158790588, Test loss 0.24797677993774414.\n",
      "Epoch 660: Train loss 0.2317858189344406, Test loss 0.24543975293636322.\n",
      "Epoch 665: Train loss 0.22989130020141602, Test loss 0.24296709895133972.\n",
      "Epoch 670: Train loss 0.22808513045310974, Test loss 0.2405579537153244.\n",
      "Epoch 675: Train loss 0.2263646274805069, Test loss 0.23820780217647552.\n",
      "Epoch 680: Train loss 0.22472797334194183, Test loss 0.23591972887516022.\n",
      "Epoch 685: Train loss 0.2231721431016922, Test loss 0.23368899524211884.\n",
      "Epoch 690: Train loss 0.22169612348079681, Test loss 0.23151551187038422.\n",
      "Epoch 695: Train loss 0.22030021250247955, Test loss 0.22939848899841309.\n",
      "Epoch 700: Train loss 0.21898257732391357, Test loss 0.22733379900455475.\n",
      "Epoch 705: Train loss 0.21774429082870483, Test loss 0.22532284259796143.\n",
      "Epoch 710: Train loss 0.21658681333065033, Test loss 0.22336608171463013.\n",
      "Epoch 715: Train loss 0.2155107855796814, Test loss 0.2214619368314743.\n",
      "Epoch 720: Train loss 0.21451561152935028, Test loss 0.21961535513401031.\n",
      "Epoch 725: Train loss 0.21360042691230774, Test loss 0.21782967448234558.\n",
      "Epoch 730: Train loss 0.21275556087493896, Test loss 0.21611104905605316.\n",
      "Epoch 735: Train loss 0.21196824312210083, Test loss 0.21446508169174194.\n",
      "Epoch 740: Train loss 0.2112215757369995, Test loss 0.2128959596157074.\n",
      "Epoch 745: Train loss 0.21049529314041138, Test loss 0.21140359342098236.\n",
      "Epoch 750: Train loss 0.2097734808921814, Test loss 0.2099868804216385.\n",
      "Epoch 755: Train loss 0.20904441177845, Test loss 0.20864245295524597.\n",
      "Epoch 760: Train loss 0.20829787850379944, Test loss 0.20735815167427063.\n",
      "Epoch 765: Train loss 0.2075260877609253, Test loss 0.2061234563589096.\n",
      "Epoch 770: Train loss 0.2067258208990097, Test loss 0.20492400228977203.\n",
      "Epoch 775: Train loss 0.20589366555213928, Test loss 0.20374786853790283.\n",
      "Epoch 780: Train loss 0.205032080411911, Test loss 0.20258662104606628.\n",
      "Epoch 785: Train loss 0.20414428412914276, Test loss 0.2014319747686386.\n",
      "Epoch 790: Train loss 0.20323696732521057, Test loss 0.20028525590896606.\n",
      "Epoch 795: Train loss 0.20231926441192627, Test loss 0.1991441398859024.\n",
      "Epoch 800: Train loss 0.20139671862125397, Test loss 0.19800949096679688.\n",
      "Epoch 805: Train loss 0.20047733187675476, Test loss 0.19688358902931213.\n",
      "Epoch 810: Train loss 0.19957251846790314, Test loss 0.19576075673103333.\n",
      "Epoch 815: Train loss 0.19868937134742737, Test loss 0.1946375072002411.\n",
      "Epoch 820: Train loss 0.19783931970596313, Test loss 0.1935020089149475.\n",
      "Epoch 825: Train loss 0.19702178239822388, Test loss 0.19233964383602142.\n",
      "Epoch 830: Train loss 0.1962292492389679, Test loss 0.19114206731319427.\n",
      "Epoch 835: Train loss 0.19544123113155365, Test loss 0.18990813195705414.\n",
      "Epoch 840: Train loss 0.1946289837360382, Test loss 0.18864890933036804.\n",
      "Epoch 845: Train loss 0.1937694400548935, Test loss 0.18738019466400146.\n",
      "Epoch 850: Train loss 0.19285708665847778, Test loss 0.1861221343278885.\n",
      "Epoch 855: Train loss 0.1919022798538208, Test loss 0.18489094078540802.\n",
      "Epoch 860: Train loss 0.19091829657554626, Test loss 0.18369542062282562.\n",
      "Epoch 865: Train loss 0.1899174451828003, Test loss 0.18253745138645172.\n",
      "Epoch 870: Train loss 0.18890678882598877, Test loss 0.1814127415418625.\n",
      "Epoch 875: Train loss 0.1878901720046997, Test loss 0.18031619489192963.\n",
      "Epoch 880: Train loss 0.18686607480049133, Test loss 0.17924614250659943.\n",
      "Epoch 885: Train loss 0.1858377456665039, Test loss 0.17819753289222717.\n",
      "Epoch 890: Train loss 0.18480968475341797, Test loss 0.17717266082763672.\n",
      "Epoch 895: Train loss 0.18377992510795593, Test loss 0.17616966366767883.\n",
      "Epoch 900: Train loss 0.182755246758461, Test loss 0.17518718540668488.\n",
      "Epoch 905: Train loss 0.1817348599433899, Test loss 0.1742255836725235.\n",
      "Epoch 910: Train loss 0.18071411550045013, Test loss 0.17328253388404846.\n",
      "Epoch 915: Train loss 0.1797032505273819, Test loss 0.17235693335533142.\n",
      "Epoch 920: Train loss 0.17869403958320618, Test loss 0.17145003378391266.\n",
      "Epoch 925: Train loss 0.1776926964521408, Test loss 0.17056062817573547.\n",
      "Epoch 930: Train loss 0.17670127749443054, Test loss 0.1696871519088745.\n",
      "Epoch 935: Train loss 0.17571434378623962, Test loss 0.16883046925067902.\n",
      "Epoch 940: Train loss 0.1747358739376068, Test loss 0.16798965632915497.\n",
      "Epoch 945: Train loss 0.17376743257045746, Test loss 0.1671629399061203.\n",
      "Epoch 950: Train loss 0.17280395328998566, Test loss 0.16635219752788544.\n",
      "Epoch 955: Train loss 0.17185162007808685, Test loss 0.16555431485176086.\n",
      "Epoch 960: Train loss 0.17090798914432526, Test loss 0.16477040946483612.\n",
      "Epoch 965: Train loss 0.1699751317501068, Test loss 0.1640002578496933.\n",
      "Epoch 970: Train loss 0.1690501570701599, Test loss 0.1632423996925354.\n",
      "Epoch 975: Train loss 0.16813576221466064, Test loss 0.1624974012374878.\n",
      "Epoch 980: Train loss 0.16723088920116425, Test loss 0.1617645025253296.\n",
      "Epoch 985: Train loss 0.1663353592157364, Test loss 0.16104352474212646.\n",
      "Epoch 990: Train loss 0.16544929146766663, Test loss 0.16033385694026947.\n",
      "Epoch 995: Train loss 0.16457447409629822, Test loss 0.15963545441627502.\n",
      "Epoch 1000: Train loss 0.1637088805437088, Test loss 0.15894782543182373.\n",
      "Epoch 1005: Train loss 0.16285115480422974, Test loss 0.15827161073684692.\n",
      "Epoch 1010: Train loss 0.1620039939880371, Test loss 0.15760494768619537.\n",
      "Epoch 1015: Train loss 0.16116541624069214, Test loss 0.1569489687681198.\n",
      "Epoch 1020: Train loss 0.1603369563817978, Test loss 0.156302347779274.\n",
      "Epoch 1025: Train loss 0.15951703488826752, Test loss 0.15566661953926086.\n",
      "Epoch 1030: Train loss 0.15870468318462372, Test loss 0.15503999590873718.\n",
      "Epoch 1035: Train loss 0.15790307521820068, Test loss 0.15442337095737457.\n",
      "Epoch 1040: Train loss 0.15710967779159546, Test loss 0.1538163423538208.\n",
      "Epoch 1045: Train loss 0.15632294118404388, Test loss 0.15321771800518036.\n",
      "Epoch 1050: Train loss 0.155546173453331, Test loss 0.15262842178344727.\n",
      "Epoch 1055: Train loss 0.1547771841287613, Test loss 0.15204881131649017.\n",
      "Epoch 1060: Train loss 0.15401636064052582, Test loss 0.15147759020328522.\n",
      "Epoch 1065: Train loss 0.1532629430294037, Test loss 0.15091615915298462.\n",
      "Epoch 1070: Train loss 0.1525162160396576, Test loss 0.15036334097385406.\n",
      "Epoch 1075: Train loss 0.15177884697914124, Test loss 0.14981980621814728.\n",
      "Epoch 1080: Train loss 0.15104758739471436, Test loss 0.14928433299064636.\n",
      "Epoch 1085: Train loss 0.15032510459423065, Test loss 0.14875808358192444.\n",
      "Epoch 1090: Train loss 0.14960940182209015, Test loss 0.14824041724205017.\n",
      "Epoch 1095: Train loss 0.14889873564243317, Test loss 0.14773182570934296.\n",
      "Epoch 1100: Train loss 0.14819815754890442, Test loss 0.14723418653011322.\n",
      "Epoch 1105: Train loss 0.14750297367572784, Test loss 0.14674368500709534.\n",
      "Epoch 1110: Train loss 0.14681512117385864, Test loss 0.1462632268667221.\n",
      "Epoch 1115: Train loss 0.14613445103168488, Test loss 0.14579342305660248.\n",
      "Epoch 1120: Train loss 0.14546188712120056, Test loss 0.14533361792564392.\n",
      "Epoch 1125: Train loss 0.14479602873325348, Test loss 0.14488433301448822.\n",
      "Epoch 1130: Train loss 0.1441376507282257, Test loss 0.14444519579410553.\n",
      "Epoch 1135: Train loss 0.14348679780960083, Test loss 0.1440190225839615.\n",
      "Epoch 1140: Train loss 0.14284254610538483, Test loss 0.1436050683259964.\n",
      "Epoch 1145: Train loss 0.14220766723155975, Test loss 0.14320479333400726.\n",
      "Epoch 1150: Train loss 0.1415819525718689, Test loss 0.14281931519508362.\n",
      "Epoch 1155: Train loss 0.14096380770206451, Test loss 0.14245128631591797.\n",
      "Epoch 1160: Train loss 0.14035503566265106, Test loss 0.14210368692874908.\n",
      "Epoch 1165: Train loss 0.1397581249475479, Test loss 0.14177843928337097.\n",
      "Epoch 1170: Train loss 0.13917246460914612, Test loss 0.14148250222206116.\n",
      "Epoch 1175: Train loss 0.13860101997852325, Test loss 0.14122702181339264.\n",
      "Epoch 1180: Train loss 0.13804475963115692, Test loss 0.14102387428283691.\n",
      "Epoch 1185: Train loss 0.13751384615898132, Test loss 0.14090284705162048.\n",
      "Epoch 1190: Train loss 0.13701027631759644, Test loss 0.14092877507209778.\n",
      "Epoch 1195: Train loss 0.13654367625713348, Test loss 0.14123918116092682.\n",
      "Epoch 1200: Train loss 0.1360289603471756, Test loss 0.14195170998573303.\n",
      "Epoch 1205: Train loss 0.13487493991851807, Test loss 0.14168216288089752.\n",
      "Epoch 1210: Train loss 0.1332056224346161, Test loss 0.13929560780525208.\n",
      "Epoch 1215: Train loss 0.13195736706256866, Test loss 0.13773271441459656.\n",
      "Epoch 1220: Train loss 0.13162311911582947, Test loss 0.13734714686870575.\n",
      "Epoch 1225: Train loss 0.13184545934200287, Test loss 0.13762924075126648.\n",
      "Epoch 1230: Train loss 0.13151291012763977, Test loss 0.1378689855337143.\n",
      "Epoch 1235: Train loss 0.13089923560619354, Test loss 0.13758817315101624.\n",
      "Epoch 1240: Train loss 0.1303400993347168, Test loss 0.13705919682979584.\n",
      "Epoch 1245: Train loss 0.1298045665025711, Test loss 0.13657967746257782.\n",
      "Epoch 1250: Train loss 0.12927398085594177, Test loss 0.1361643522977829.\n",
      "Epoch 1255: Train loss 0.12876974046230316, Test loss 0.1357390433549881.\n",
      "Epoch 1260: Train loss 0.1282716989517212, Test loss 0.13527639210224152.\n",
      "Epoch 1265: Train loss 0.12777234613895416, Test loss 0.13478226959705353.\n",
      "Epoch 1270: Train loss 0.12728668749332428, Test loss 0.13428997993469238.\n",
      "Epoch 1275: Train loss 0.12682151794433594, Test loss 0.13382969796657562.\n",
      "Epoch 1280: Train loss 0.12636880576610565, Test loss 0.13339681923389435.\n",
      "Epoch 1285: Train loss 0.12591876089572906, Test loss 0.13296964764595032.\n",
      "Epoch 1290: Train loss 0.12546640634536743, Test loss 0.13252267241477966.\n",
      "Epoch 1295: Train loss 0.12501035630702972, Test loss 0.13205038011074066.\n",
      "Epoch 1300: Train loss 0.12455269694328308, Test loss 0.1315552294254303.\n",
      "Epoch 1305: Train loss 0.1241021603345871, Test loss 0.13104940950870514.\n",
      "Epoch 1310: Train loss 0.12366008758544922, Test loss 0.13054639101028442.\n",
      "Epoch 1315: Train loss 0.12322676926851273, Test loss 0.13005003333091736.\n",
      "Epoch 1320: Train loss 0.12280160933732986, Test loss 0.12956221401691437.\n",
      "Epoch 1325: Train loss 0.12238304316997528, Test loss 0.12908560037612915.\n",
      "Epoch 1330: Train loss 0.1219663992524147, Test loss 0.1286081224679947.\n",
      "Epoch 1335: Train loss 0.12155184149742126, Test loss 0.12813083827495575.\n",
      "Epoch 1340: Train loss 0.12113643437623978, Test loss 0.12764933705329895.\n",
      "Epoch 1345: Train loss 0.12072045356035233, Test loss 0.12716124951839447.\n",
      "Epoch 1350: Train loss 0.12030475586652756, Test loss 0.1266690343618393.\n",
      "Epoch 1355: Train loss 0.11989082396030426, Test loss 0.12618009746074677.\n",
      "Epoch 1360: Train loss 0.11947563290596008, Test loss 0.1256914585828781.\n",
      "Epoch 1365: Train loss 0.11906106770038605, Test loss 0.1252116858959198.\n",
      "Epoch 1370: Train loss 0.11864518374204636, Test loss 0.1247352883219719.\n",
      "Epoch 1375: Train loss 0.11822428554296494, Test loss 0.12425770610570908.\n",
      "Epoch 1380: Train loss 0.11779632419347763, Test loss 0.1237831637263298.\n",
      "Epoch 1385: Train loss 0.11735834181308746, Test loss 0.12331002205610275.\n",
      "Epoch 1390: Train loss 0.11690280586481094, Test loss 0.12283959984779358.\n",
      "Epoch 1395: Train loss 0.11642424762248993, Test loss 0.12237077951431274.\n",
      "Epoch 1400: Train loss 0.1159108579158783, Test loss 0.12191090732812881.\n",
      "Epoch 1405: Train loss 0.1153501495718956, Test loss 0.1214708760380745.\n",
      "Epoch 1410: Train loss 0.11472541093826294, Test loss 0.12106898427009583.\n",
      "Epoch 1415: Train loss 0.11404268443584442, Test loss 0.12074162065982819.\n",
      "Epoch 1420: Train loss 0.11335504800081253, Test loss 0.1205168217420578.\n",
      "Epoch 1425: Train loss 0.11275311559438705, Test loss 0.1204790323972702.\n",
      "Epoch 1430: Train loss 0.1120053082704544, Test loss 0.12189639359712601.\n",
      "Epoch 1435: Train loss 0.11382757127285004, Test loss 0.14104467630386353.\n",
      "Epoch 1440: Train loss 0.11551811546087265, Test loss 0.14504291117191315.\n",
      "Epoch 1445: Train loss 0.11913848668336868, Test loss 0.15193921327590942.\n",
      "Epoch 1450: Train loss 0.12164025008678436, Test loss 0.15868233144283295.\n",
      "Epoch 1455: Train loss 0.12082143872976303, Test loss 0.15539096295833588.\n",
      "Epoch 1460: Train loss 0.12006106972694397, Test loss 0.15181390941143036.\n",
      "Epoch 1465: Train loss 0.12410595268011093, Test loss 0.15753652155399323.\n",
      "Epoch 1470: Train loss 0.1262926161289215, Test loss 0.15649667382240295.\n",
      "Epoch 1475: Train loss 0.1272791624069214, Test loss 0.1535646766424179.\n",
      "Epoch 1480: Train loss 0.12776093184947968, Test loss 0.1515471190214157.\n",
      "Epoch 1485: Train loss 0.12785276770591736, Test loss 0.1505264937877655.\n",
      "Epoch 1490: Train loss 0.12767021358013153, Test loss 0.14986631274223328.\n",
      "Epoch 1495: Train loss 0.12742109596729279, Test loss 0.149246945977211.\n",
      "Epoch 1500: Train loss 0.12718595564365387, Test loss 0.14858180284500122.\n",
      "Epoch 1505: Train loss 0.1269981414079666, Test loss 0.14793068170547485.\n",
      "Epoch 1510: Train loss 0.12680841982364655, Test loss 0.14726921916007996.\n",
      "Epoch 1515: Train loss 0.1266326755285263, Test loss 0.1466366946697235.\n",
      "Epoch 1520: Train loss 0.12644556164741516, Test loss 0.14600101113319397.\n",
      "Epoch 1525: Train loss 0.12625885009765625, Test loss 0.14537011086940765.\n",
      "Epoch 1530: Train loss 0.12608110904693604, Test loss 0.1447581797838211.\n",
      "Epoch 1535: Train loss 0.1259048879146576, Test loss 0.1441570520401001.\n",
      "Epoch 1540: Train loss 0.12572437524795532, Test loss 0.1435573846101761.\n",
      "Epoch 1545: Train loss 0.12554143369197845, Test loss 0.14295841753482819.\n",
      "Epoch 1550: Train loss 0.12536212801933289, Test loss 0.14237269759178162.\n",
      "Epoch 1555: Train loss 0.12518224120140076, Test loss 0.14178454875946045.\n",
      "Epoch 1560: Train loss 0.1250043660402298, Test loss 0.14120781421661377.\n",
      "Epoch 1565: Train loss 0.12482725828886032, Test loss 0.14062811434268951.\n",
      "Epoch 1570: Train loss 0.12464621663093567, Test loss 0.1400449275970459.\n",
      "Epoch 1575: Train loss 0.12446314096450806, Test loss 0.13948862254619598.\n",
      "Epoch 1580: Train loss 0.12427608668804169, Test loss 0.13889959454536438.\n",
      "Epoch 1585: Train loss 0.12410064786672592, Test loss 0.13831888139247894.\n",
      "Epoch 1590: Train loss 0.12392289936542511, Test loss 0.13773584365844727.\n",
      "Epoch 1595: Train loss 0.12373531609773636, Test loss 0.13715356588363647.\n",
      "Epoch 1600: Train loss 0.12354996055364609, Test loss 0.13653777539730072.\n",
      "Epoch 1605: Train loss 0.12336437404155731, Test loss 0.1359017789363861.\n",
      "Epoch 1610: Train loss 0.12317167222499847, Test loss 0.13523206114768982.\n",
      "Epoch 1615: Train loss 0.12295947968959808, Test loss 0.13446836173534393.\n",
      "Epoch 1620: Train loss 0.1226964071393013, Test loss 0.1335080862045288.\n",
      "Epoch 1625: Train loss 0.12216459214687347, Test loss 0.1318766474723816.\n",
      "Epoch 1630: Train loss 0.11742470413446426, Test loss 0.12494779378175735.\n",
      "Epoch 1635: Train loss 0.11013085395097733, Test loss 0.11382363736629486.\n",
      "Epoch 1640: Train loss 0.10175241529941559, Test loss 0.11197278648614883.\n",
      "Epoch 1645: Train loss 0.10103809088468552, Test loss 0.11013462394475937.\n",
      "Epoch 1650: Train loss 0.10140316188335419, Test loss 0.10942589491605759.\n",
      "Epoch 1655: Train loss 0.1011631190776825, Test loss 0.10894589871168137.\n",
      "Epoch 1660: Train loss 0.10086292028427124, Test loss 0.10846658051013947.\n",
      "Epoch 1665: Train loss 0.10057974606752396, Test loss 0.10801712423563004.\n",
      "Epoch 1670: Train loss 0.10029922425746918, Test loss 0.10758693516254425.\n",
      "Epoch 1675: Train loss 0.10003085434436798, Test loss 0.10718085616827011.\n",
      "Epoch 1680: Train loss 0.09976658225059509, Test loss 0.10679147392511368.\n",
      "Epoch 1685: Train loss 0.09950631856918335, Test loss 0.10641437768936157.\n",
      "Epoch 1690: Train loss 0.09926223754882812, Test loss 0.10605242848396301.\n",
      "Epoch 1695: Train loss 0.09901338815689087, Test loss 0.10569882392883301.\n",
      "Epoch 1700: Train loss 0.0987774208188057, Test loss 0.1053544208407402.\n",
      "Epoch 1705: Train loss 0.09853924065828323, Test loss 0.10501912236213684.\n",
      "Epoch 1710: Train loss 0.09829854220151901, Test loss 0.10468745231628418.\n",
      "Epoch 1715: Train loss 0.09807660430669785, Test loss 0.10436659306287766.\n",
      "Epoch 1720: Train loss 0.09785356372594833, Test loss 0.1040530875325203.\n",
      "Epoch 1725: Train loss 0.0976378321647644, Test loss 0.1037450060248375.\n",
      "Epoch 1730: Train loss 0.0974234938621521, Test loss 0.10344292968511581.\n",
      "Epoch 1735: Train loss 0.09720966964960098, Test loss 0.10314574837684631.\n",
      "Epoch 1740: Train loss 0.09700171649456024, Test loss 0.10285356640815735.\n",
      "Epoch 1745: Train loss 0.09679354727268219, Test loss 0.10256606340408325.\n",
      "Epoch 1750: Train loss 0.09658975899219513, Test loss 0.10228423774242401.\n",
      "Epoch 1755: Train loss 0.09639552980661392, Test loss 0.10200809687376022.\n",
      "Epoch 1760: Train loss 0.09619177877902985, Test loss 0.10173329710960388.\n",
      "Epoch 1765: Train loss 0.0960008054971695, Test loss 0.10146541148424149.\n",
      "Epoch 1770: Train loss 0.09580789506435394, Test loss 0.10120028257369995.\n",
      "Epoch 1775: Train loss 0.09561917930841446, Test loss 0.10093975067138672.\n",
      "Epoch 1780: Train loss 0.09543382376432419, Test loss 0.10068406909704208.\n",
      "Epoch 1785: Train loss 0.09525274485349655, Test loss 0.10043127089738846.\n",
      "Epoch 1790: Train loss 0.09506794065237045, Test loss 0.10018134862184525.\n",
      "Epoch 1795: Train loss 0.09488506615161896, Test loss 0.0999346598982811.\n",
      "Epoch 1800: Train loss 0.09470874816179276, Test loss 0.09969154000282288.\n",
      "Epoch 1805: Train loss 0.09453807026147842, Test loss 0.09945326298475266.\n",
      "Epoch 1810: Train loss 0.09435894340276718, Test loss 0.09921465069055557.\n",
      "Epoch 1815: Train loss 0.09418657422065735, Test loss 0.09898214042186737.\n",
      "Epoch 1820: Train loss 0.0940181240439415, Test loss 0.09875176101922989.\n",
      "Epoch 1825: Train loss 0.09385387599468231, Test loss 0.09852731972932816.\n",
      "Epoch 1830: Train loss 0.09369159489870071, Test loss 0.09830295294523239.\n",
      "Epoch 1835: Train loss 0.09352332353591919, Test loss 0.09807842969894409.\n",
      "Epoch 1840: Train loss 0.09336525946855545, Test loss 0.09786208719015121.\n",
      "Epoch 1845: Train loss 0.09319917857646942, Test loss 0.09764253348112106.\n",
      "Epoch 1850: Train loss 0.09303650259971619, Test loss 0.09742637723684311.\n",
      "Epoch 1855: Train loss 0.09288204461336136, Test loss 0.09721573442220688.\n",
      "Epoch 1860: Train loss 0.09272241592407227, Test loss 0.09700710326433182.\n",
      "Epoch 1865: Train loss 0.0925670638680458, Test loss 0.09679686278104782.\n",
      "Epoch 1870: Train loss 0.09242115914821625, Test loss 0.09659680724143982.\n",
      "Epoch 1875: Train loss 0.09226495772600174, Test loss 0.09638955444097519.\n",
      "Epoch 1880: Train loss 0.09210970252752304, Test loss 0.09618809074163437.\n",
      "Epoch 1885: Train loss 0.09196166694164276, Test loss 0.09598985314369202.\n",
      "Epoch 1890: Train loss 0.09181497246026993, Test loss 0.09579231590032578.\n",
      "Epoch 1895: Train loss 0.09166223555803299, Test loss 0.09559477865695953.\n",
      "Epoch 1900: Train loss 0.0915178656578064, Test loss 0.095400869846344.\n",
      "Epoch 1905: Train loss 0.09136754274368286, Test loss 0.09520690888166428.\n",
      "Epoch 1910: Train loss 0.0912228673696518, Test loss 0.0950135663151741.\n",
      "Epoch 1915: Train loss 0.09107149392366409, Test loss 0.09482114017009735.\n",
      "Epoch 1920: Train loss 0.09092596173286438, Test loss 0.09463158249855042.\n",
      "Epoch 1925: Train loss 0.09078193455934525, Test loss 0.09444426745176315.\n",
      "Epoch 1930: Train loss 0.09063731878995895, Test loss 0.09425535798072815.\n",
      "Epoch 1935: Train loss 0.09048673510551453, Test loss 0.09406730532646179.\n",
      "Epoch 1940: Train loss 0.09034484624862671, Test loss 0.0938820093870163.\n",
      "Epoch 1945: Train loss 0.09020016342401505, Test loss 0.093695268034935.\n",
      "Epoch 1950: Train loss 0.09005321562290192, Test loss 0.09350947290658951.\n",
      "Epoch 1955: Train loss 0.08990427106618881, Test loss 0.09332158416509628.\n",
      "Epoch 1960: Train loss 0.08976180106401443, Test loss 0.09313911199569702.\n",
      "Epoch 1965: Train loss 0.08961477130651474, Test loss 0.09295471757650375.\n",
      "Epoch 1970: Train loss 0.08947106450796127, Test loss 0.09277109056711197.\n",
      "Epoch 1975: Train loss 0.0893218070268631, Test loss 0.09258588403463364.\n",
      "Epoch 1980: Train loss 0.08918168395757675, Test loss 0.09240308403968811.\n",
      "Epoch 1985: Train loss 0.0890311598777771, Test loss 0.09221779555082321.\n",
      "Epoch 1990: Train loss 0.08888580650091171, Test loss 0.09203445166349411.\n",
      "Epoch 1995: Train loss 0.08873648941516876, Test loss 0.09184948354959488.\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "file = tf.io.read_file(\"../exps/demo5_train_test_data.txt\")\n",
    "train_test_data = tf.io.parse_tensor(file, out_type=tf.float32)\n",
    "train_test_data = tf.random.shuffle(train_test_data)\n",
    "train_data = train_test_data[:num_train, :]\n",
    "test_data = train_test_data[num_train:, :]\n",
    "print(train_data.shape, test_data.shape)\n",
    "lhnn = LatentHamiltonianNeuralNetwork(3, 100, 10)\n",
    "lhnn.build(input_shape=(1, 20))\n",
    "train_hist, test_hist = lhnn.train(2000, 1000, 5e-5, train_data, test_data, save_dir=\"../exps/demo5_lhnn.weights.h5\", print_every=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcbo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
